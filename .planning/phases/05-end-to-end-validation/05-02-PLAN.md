---
phase: 05-end-to-end-validation
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/validation/comparators.jl
  - test/validation/test_comparators.jl
autonomous: true

must_haves:
  truths:
    - "Total cost comparison returns PASS when actual is within 5% of expected, FAIL otherwise"
    - "PLD comparison returns PASS when configurable pass-rate % of periods are within tolerance %, FAIL otherwise"
    - "Dispatch comparison returns per-plant PASS/FAIL at per-period granularity with configurable tolerance"
    - "All comparators handle edge cases: zero expected values, empty DataFrames, missing plants"
    - "FC submarket excluded from PLD validation by default"
  artifacts:
    - path: "src/validation/comparators.jl"
      provides: "compare_total_cost, compare_pld, compare_dispatch"
      contains: "function compare_total_cost"
    - path: "test/validation/test_comparators.jl"
      provides: "Unit tests for all comparators"
      min_lines: 120
  key_links:
    - from: "src/validation/comparators.jl"
      to: "src/validation/validation_types.jl"
      via: "Returns MetricComparison, PLDComparison, DispatchComparison structs"
      pattern: "MetricComparison\\("
    - from: "src/validation/comparators.jl"
      to: "DataFrames.jl"
      via: "innerjoin for matching actual vs expected data"
      pattern: "innerjoin"
---

<objective>
Create the comparison engine that evaluates OpenDESSEM results against reference data.

Purpose: The comparators are the core logic of validation -- they determine pass/fail for each metric type. This is pure business logic with well-defined I/O, ideal for TDD. Each comparator takes actual values + reference values + tolerance parameters and returns a MetricComparison struct.

Output:
- `compare_total_cost()` - compares single cost value with relative tolerance
- `compare_pld()` - compares PLD DataFrames using pass-rate threshold approach
- `compare_dispatch()` - compares per-plant per-period dispatch DataFrames
- All comparators return MetricComparison structs with detailed diagnostic information
- Edge case handling for division by zero, missing data, FC submarket
</objective>

<execution_context>
@/home/pedro/.claude/get-shit-done/workflows/execute-plan.md
@/home/pedro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-end-to-end-validation/05-CONTEXT.md
@.planning/phases/05-end-to-end-validation/05-RESEARCH.md
@src/validation/validation_types.jl (types defined in Plan 01 -- if not yet available, define locally for testing)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cost and PLD comparators with TDD</name>
  <files>
    src/validation/comparators.jl
    test/validation/test_comparators.jl
  </files>
  <action>
**RED phase: Write tests first in `test/validation/test_comparators.jl`.**

Note: This plan runs in Wave 1 parallel with Plan 01. The test file should include validation_types.jl directly (from `src/validation/validation_types.jl`). If Plan 01 hasn't completed yet, the test can define minimal stub types inline -- but the implementation must use the real types from validation_types.jl.

Create tests for:

**1. `compare_total_cost(actual_cost, expected_cost, tolerance)` -> MetricComparison:**
- Test: actual=100.0, expected=100.0, tolerance=0.05 => :passed, relative_diff=0.0
- Test: actual=104.0, expected=100.0, tolerance=0.05 => :passed (4% < 5%)
- Test: actual=106.0, expected=100.0, tolerance=0.05 => :failed (6% > 5%)
- Test: actual=95.0, expected=100.0, tolerance=0.05 => :passed (5% = 5%, boundary)
- Test: expected=0.0 => handle gracefully (don't divide by zero). If expected is zero and actual is also zero, pass. If expected is zero and actual is nonzero, fail.
- Test: returns MetricComparison with metric_type=:cost, correct expected/actual values

**2. `compare_pld(actual_df, reference_df, pass_rate_threshold, tolerance; exclude_fc=true)` -> MetricComparison:**
- Test: all periods match exactly => pass_rate=1.0, :passed
- Test: 9/10 periods within tolerance, threshold=0.80 => pass_rate=0.9, :passed
- Test: 7/10 periods within tolerance, threshold=0.80 => pass_rate=0.7, :failed
- Test: FC submarket excluded by default (create data with SE, S, NE, N, FC; verify FC rows not in comparison count)
- Test: exclude_fc=false includes FC submarket
- Test: zero expected PLD handling (both zero => pass, nonzero vs zero => fail)
- Test: empty DataFrames => :skipped with reason
- Test: returns MetricComparison with details as Vector of PLDComparison items
- Test: PLDComparison items have correct submarket, period, expected, actual, passed

Actual DataFrame schema from `get_pld_dataframe()`: columns `submarket`, `period`, `pld` (R$/MWh).
Reference DataFrame schema from `load_reference_pld()`: columns `period`, `submarket`, `pld_rs_per_mwh`.

The comparator must handle column name differences: rename reference `pld_rs_per_mwh` to match actual `pld`, or join on `:period, :submarket` and access the appropriate columns.

Default parameter values per Claude's discretion: `pass_rate_threshold=0.80`, `tolerance=0.10`.

**GREEN phase: Implement `compare_total_cost` and `compare_pld` in `src/validation/comparators.jl`.**

For `compare_total_cost`:
- Use safe relative difference: `abs(actual - expected) / max(abs(expected), 1e-10)`
- Return MetricComparison with all fields populated

For `compare_pld`:
- Filter out FC submarket if `exclude_fc=true` (filter rows where submarket != "FC")
- Join actual and reference DataFrames on `:submarket` and `:period`
- For each joined row, compute relative difference with zero-safe denominator
- Track PLDComparison for each row
- Compute pass_rate = passed / total
- Return MetricComparison with details populated

**REFACTOR:** Ensure docstrings, parameter documentation, and edge case comments.
  </action>
  <verify>
Run: `julia --project=test -e 'include("test/validation/test_comparators.jl")'`
All cost and PLD comparator tests pass. At least 25 @test assertions.
  </verify>
  <done>
compare_total_cost() correctly applies relative tolerance with zero-safe division. compare_pld() correctly implements pass-rate threshold approach with FC submarket exclusion, zero-value handling, and per-period PLDComparison detail items. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add dispatch comparator with TDD</name>
  <files>
    src/validation/comparators.jl
    test/validation/test_comparators.jl
  </files>
  <action>
**RED phase: Add dispatch tests to `test/validation/test_comparators.jl`.**

**3. `compare_dispatch(actual_df, reference_df, tolerance; plant_type=:thermal)` -> Vector{MetricComparison}:**
- Returns one MetricComparison per unique plant_id (per-plant granularity per CONTEXT.md)
- Each MetricComparison contains details as Vector of DispatchComparison items (per-period)

Tests:
- Test: single plant, all periods match => one MetricComparison with :passed, pass_rate=1.0
- Test: single plant, some periods off => MetricComparison with :failed if pass_rate below default threshold
- Test: multiple plants => returns vector with one MetricComparison per plant
- Test: plant in reference but not in actual => :skipped with reason "Plant X not found in actual"
- Test: plant in actual but not in reference => ignored (only compare plants in reference)
- Test: zero expected dispatch => handle gracefully (if both zero, pass; if only expected zero and actual < 1.0 MW, pass)
- Test: default tolerance=0.10 (10%)
- Test: DispatchComparison items have correct plant_id, period, expected, actual, passed

Actual DataFrame schema for thermal: columns `plant_id`, `period`, `generation` (MW).
Reference DataFrame schema: columns `period`, `plant_id`, `generation_mw`.

The comparator must handle column name differences: reference uses `generation_mw`, actual uses `generation`.

**4. `compare_all_dispatch(actual_thermal_df, actual_hydro_df, ref_thermal_df, ref_hydro_df, tolerance)` -> Vector{MetricComparison}:**
- Convenience wrapper that calls compare_dispatch for both thermal and hydro
- Concatenates results
- Test: returns combined results from thermal and hydro

Default parameter value per Claude's discretion: `pass_rate_threshold=0.80`, `tolerance=0.10`.

**GREEN phase: Implement `compare_dispatch` and `compare_all_dispatch` in `src/validation/comparators.jl`.**

For `compare_dispatch`:
- Group reference DataFrame by plant_id
- For each plant, find matching rows in actual DataFrame
- If plant not found in actual, create skipped MetricComparison
- Otherwise, join on period, compute per-period relative difference
- Track DispatchComparison for each period
- Compute pass_rate per plant
- Return MetricComparison per plant

For zero dispatch: use threshold `abs(actual - expected) < max(abs(expected) * tolerance, 1.0)` to handle near-zero values with an absolute floor of 1.0 MW.

**REFACTOR:** Factor out shared relative difference logic into a helper `safe_relative_diff(actual, expected; epsilon=1e-10)`.
  </action>
  <verify>
Run: `julia --project=test -e 'include("test/validation/test_comparators.jl")'`
All comparator tests pass (cost + PLD + dispatch). At least 50 @test assertions total.
  </verify>
  <done>
All three comparators (cost, PLD, dispatch) work correctly. compare_dispatch returns per-plant MetricComparisons with per-period DispatchComparison details. safe_relative_diff helper handles zero-value edge cases. FC submarket excluded from PLD by default. At least 50 test assertions pass.
  </done>
</task>

</tasks>

<verification>
1. `julia --project=test -e 'include("test/validation/test_comparators.jl")'` -- all pass
2. compare_total_cost handles: exact match, within tolerance, exceeded tolerance, zero expected
3. compare_pld handles: all pass, pass-rate threshold, FC exclusion, empty data
4. compare_dispatch handles: per-plant, per-period, missing plants, zero dispatch
5. All comparators return MetricComparison structs with correct fields
6. safe_relative_diff helper prevents division by zero
</verification>

<success_criteria>
- compare_total_cost() correctly evaluates 5% relative tolerance with zero-safe division
- compare_pld() implements pass-rate threshold approach (not correlation) per CONTEXT.md
- compare_pld() excludes FC submarket by default per research findings
- compare_dispatch() returns per-plant per-period results per CONTEXT.md
- All comparators handle edge cases (zero values, empty DataFrames, missing plants)
- All comparators return MetricComparison structs with detailed diagnostic information
- 50+ test assertions pass across all comparator types
</success_criteria>

<output>
After completion, create `.planning/phases/05-end-to-end-validation/05-02-SUMMARY.md`
</output>
