---
phase: 05-end-to-end-validation
plan: 03
type: execute
wave: 3
depends_on: ["05-01", "05-02"]
files_modified:
  - src/validation/Validation.jl
  - src/validation/reporters.jl
  - src/validation/dispatch_builders.jl
  - src/analysis/Analysis.jl
  - test/validation/test_reporters.jl
  - test/runtests.jl
autonomous: true

must_haves:
  truths:
    - "Console report prints sectioned summary with pass/fail per metric type (cost, PLD, dispatch)"
    - "Markdown report generates complete comparison tables with expected vs actual values for ALL metrics"
    - "JSON report generates machine-readable structured output mirroring markdown content"
    - "Validation module is wired into Analysis module and exports all public functions"
    - "validate_against_reference() orchestrates all comparisons and returns ValidationResult"
    - "report_validation() generates all three formats in a single call"
    - "Dispatch DataFrame helpers iterate over system plants to build long-format DataFrames from per-plant solution extraction API"
  artifacts:
    - path: "src/validation/reporters.jl"
      provides: "print_console_report, write_markdown_report, write_json_report, report_validation"
      contains: "function print_console_report"
    - path: "src/validation/dispatch_builders.jl"
      provides: "get_thermal_dispatch_dataframe, get_hydro_dispatch_dataframe"
      contains: "function get_thermal_dispatch_dataframe"
    - path: "src/validation/Validation.jl"
      provides: "Validation module definition with all includes and exports"
      contains: "module Validation"
    - path: "test/validation/test_reporters.jl"
      provides: "Unit tests for all three report formats"
      min_lines: 80
  key_links:
    - from: "src/validation/Validation.jl"
      to: "src/validation/validation_types.jl"
      via: "include and re-export"
      pattern: "include.*validation_types"
    - from: "src/validation/Validation.jl"
      to: "src/validation/comparators.jl"
      via: "include and re-export"
      pattern: "include.*comparators"
    - from: "src/validation/Validation.jl"
      to: "src/validation/reporters.jl"
      via: "include and re-export"
      pattern: "include.*reporters"
    - from: "src/validation/dispatch_builders.jl"
      to: "src/solvers/solution_extraction.jl"
      via: "Calls get_thermal_generation(result, plant_id, time_periods) per plant"
      pattern: "get_thermal_generation.*plant"
    - from: "src/analysis/Analysis.jl"
      to: "src/validation/Validation.jl"
      via: "include and using .Validation"
      pattern: "include.*Validation"
---

<objective>
Create the multi-format reporting system and wire the Validation module together.

Purpose: Reports are how users and CI systems consume validation results. This plan creates the three required output formats (console, markdown, JSON), the main `validate_against_reference()` orchestrator function, and wires everything into the OpenDESSEM module tree via `Analysis.Validation`.

Output:
- Console reporter: sectioned summary with pass/fail per metric (not verbose)
- Markdown reporter: complete comparison tables with expected vs actual for ALL metrics
- JSON reporter: structured output mirroring markdown content
- `validate_against_reference()` function: end-to-end validation orchestrator
- `report_validation()` function: generates all three report formats
- Validation module wired into Analysis and exported through OpenDESSEM
- All existing tests continue to pass
</objective>

<execution_context>
@/home/pedro/.claude/get-shit-done/workflows/execute-plan.md
@/home/pedro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-end-to-end-validation/05-CONTEXT.md
@.planning/phases/05-end-to-end-validation/05-RESEARCH.md
@.planning/phases/05-end-to-end-validation/05-01-SUMMARY.md (types and loaders)
@.planning/phases/05-end-to-end-validation/05-02-SUMMARY.md (comparators)
@src/analysis/Analysis.jl (module to extend with Validation submodule)
@src/analysis/constraint_violations.jl (pattern: write_violation_report for report output)
@src/analysis/solution_exporter.jl (pattern: export_csv/export_json for file output)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create reporters and validate_against_reference orchestrator</name>
  <files>
    src/validation/reporters.jl
    src/validation/Validation.jl
    test/validation/test_reporters.jl
  </files>
  <action>
**Create reporters in `src/validation/reporters.jl`:**

1. **`print_console_report(vr::ValidationResult; io::IO=stdout)`**:
   - Print sectioned summary per CONTEXT.md: "One section per metric type, each section shows pass/fail status, not verbose"
   - Format:
     ```
     === OpenDESSEM Validation Report ===

     --- Total Cost ---
       Status: PASS
       Expected: R$ 1,234,567.89
       Actual:   R$ 1,240,000.00
       Tolerance: 5.0%

     --- PLD Marginal Prices ---
       Status: PASS
       Pass Rate: 92.0% (threshold: 80.0%)
       Periods Compared: 300
       Tolerance: 10.0%

     --- Dispatch (Thermal) ---
       Status: PASS
       Plants Compared: 45
       Plants Passing: 43

     --- Dispatch (Hydro) ---
       Status: PASS
       Plants Compared: 120
       Plants Passing: 118

     === Overall: PASS (exit code: 0) ===
     ```
   - For FAIL sections, show deltas: expected, actual, difference per CONTEXT.md: "Display expected value, actual value, and difference"
   - Use `@printf` for formatted numeric output

2. **`write_markdown_report(vr::ValidationResult, filepath::String)`**:
   - Generate complete Markdown report per CONTEXT.md: "Full comparison tables for ALL metrics regardless of pass/fail"
   - Sections:
     - Header with timestamp, reference directory, overall pass/fail
     - Cost section: expected vs actual, tolerance, delta
     - PLD section: table with columns | Period | Submarket | Expected | Actual | Diff | Status |
     - Thermal dispatch section: table per plant with | Period | Expected MW | Actual MW | Diff | Status |
     - Hydro dispatch section: same as thermal
     - Summary section with overall pass/fail and exit code
   - Write to file using `open(filepath, "w")`

3. **`write_json_report(vr::ValidationResult, filepath::String)`**:
   - Generate structured JSON per CONTEXT.md: "Machine-readable, mirrors markdown content in JSON structure"
   - Structure:
     ```json
     {
       "timestamp": "2026-02-17T12:00:00",
       "reference_dir": "...",
       "overall_passed": true,
       "exit_code": 0,
       "cost": { "status": "passed", "expected": ..., "actual": ..., "tolerance": ..., "relative_diff": ... },
       "pld": { "status": "passed", "pass_rate": ..., "threshold": ..., "tolerance": ..., "comparisons": [...] },
       "dispatch": { "thermal": [...], "hydro": [...] }
     }
     ```
   - Use JSON3.write for serialization

4. **`report_validation(vr::ValidationResult; output_dir::String=".")`**:
   - Convenience function per research pattern: calls all three reporters
   - Returns `(md_path, json_path)` tuple
   - Prints console to stdout, writes markdown and JSON to output_dir

5. **`validate_against_reference()` orchestrator:**
   This is the main entry point for the validation module. It:
   - Takes `result::SolverResult`, `system::ElectricitySystem`, `reference_dir::String`
   - Takes keyword args for tolerances: `cost_tolerance=0.05`, `pld_pass_rate=0.80`, `pld_tolerance=0.10`, `dispatch_tolerance=0.10`, `exclude_fc=true`, `format=:csv`
   - Loads reference data using loaders from reference_loader.jl
   - Extracts actual data from SolverResult using:
     - Cost: `get_cost_breakdown(result, system).total` (from Solvers module)
     - PLD: `get_pld_dataframe(result)` (from Solvers module)
     - Dispatch: `get_thermal_dispatch_dataframe(result, system)` and `get_hydro_dispatch_dataframe(result, system)` (from dispatch_builders.jl, see below)
   - Runs all comparators from comparators.jl
   - Collects all MetricComparison results (never early-terminates)
   - Constructs and returns ValidationResult
   - Note: This function depends on Solvers module types. Import from `...Solvers` (three dots: Validation -> Analysis -> OpenDESSEM -> Solvers)

**IMPORTANT: Create dispatch DataFrame helpers in `src/validation/dispatch_builders.jl`:**

The solution extraction API (`get_thermal_generation`, `get_hydro_generation`) returns `Vector{Float64}` for a **single plant** with signature:
```julia
get_thermal_generation(result::SolverResult, plant_id::String, time_periods::UnitRange{Int}) -> Vector{Float64}
```

There is NO bulk dispatch DataFrame API. Create two helper functions that iterate over system plants and build long-format DataFrames:

```julia
"""
    get_thermal_dispatch_dataframe(result::SolverResult, system::ElectricitySystem) -> DataFrame

Build a long-format DataFrame of thermal dispatch by iterating over all thermal plants
and calling get_thermal_generation() per plant.

Returns DataFrame with columns: plant_id::String, period::Int, generation_mw::Float64
"""
function get_thermal_dispatch_dataframe(result::SolverResult, system::ElectricitySystem)
    rows = NamedTuple{(:plant_id, :period, :generation_mw), Tuple{String, Int, Float64}}[]
    # Infer time_periods from result.variables[:thermal_generation] keys
    time_periods = _infer_time_periods(result, :thermal_generation)
    isnothing(time_periods) && return DataFrame(plant_id=String[], period=Int[], generation_mw=Float64[])

    for plant in system.thermal_plants
        gen = get_thermal_generation(result, plant.id, time_periods)
        for (i, t) in enumerate(time_periods)
            push!(rows, (plant_id=plant.id, period=t, generation_mw=gen[i]))
        end
    end
    return DataFrame(rows)
end
```

Similarly for `get_hydro_dispatch_dataframe(result, system)` using `get_hydro_generation()` and `:hydro_generation`.

Add `_infer_time_periods(result, variable_key)` private helper that extracts the time period range from `result.variables[variable_key]` dictionary keys (same pattern used in `get_cost_breakdown`).

These helpers standardize column names to `plant_id, period, generation_mw` matching the reference data schema from Plan 01's loaders.

**Create Validation module in `src/validation/Validation.jl`:**

```julia
module Validation

using CSV
using DataFrames
using JSON3
using Dates
using Printf
using Statistics

# Import SolverResult and ElectricitySystem from parent modules.
# Module nesting: Validation is inside Analysis, which is inside OpenDESSEM.
# So we need three dots to reach the OpenDESSEM level, then access Solvers/Core.
# However, ElectricitySystem is NOT in a Core submodule -- it is exported directly
# from OpenDESSEM (defined in src/core/electricity_system.jl, included at top level).
# SolverResult is in the Solvers submodule.
import ...Solvers: SolverResult, get_thermal_generation, get_hydro_generation,
    get_pld_dataframe, get_cost_breakdown, CostBreakdown
using ...OpenDESSEM: ElectricitySystem

include("validation_types.jl")
include("reference_loader.jl")
include("comparators.jl")
include("dispatch_builders.jl")
include("reporters.jl")

export
    # Types
    ValidationResult,
    MetricComparison,
    PLDComparison,
    DispatchComparison,
    # Core functions
    validate_against_reference,
    validation_exit_code,
    # Dispatch DataFrame helpers
    get_thermal_dispatch_dataframe,
    get_hydro_dispatch_dataframe,
    # Reference loaders
    load_reference_cost,
    load_reference_pld,
    load_reference_thermal_dispatch,
    load_reference_hydro_dispatch,
    load_reference_hydro_storage,
    # Comparators
    compare_total_cost,
    compare_pld,
    compare_dispatch,
    compare_all_dispatch,
    # Reporters
    print_console_report,
    write_markdown_report,
    write_json_report,
    report_validation

end # module
```

**NOTE on import paths:** The Validation module is nested as `OpenDESSEM.Analysis.Validation`. To import from sibling submodules:
- `...Solvers` means: go up from Validation (one dot) -> Analysis (two dots) -> OpenDESSEM (three dots) -> then access Solvers
- `...OpenDESSEM` to access top-level exports like ElectricitySystem

If the three-dot import for `ElectricitySystem` doesn't work (depends on how `electricity_system.jl` is included), the executor should fall back to: `import ...OpenDESSEM: ElectricitySystem` or access it via `Main.OpenDESSEM.ElectricitySystem`. The key point is there is NO `Core` submodule -- `ElectricitySystem` is defined at the OpenDESSEM top level.

**Create reporter tests in `test/validation/test_reporters.jl`:**

Tests using pre-constructed ValidationResult objects:
- Test: print_console_report produces output with "Total Cost", "PLD", "Dispatch" sections
- Test: print_console_report shows PASS/FAIL correctly
- Test: print_console_report shows deltas for failed metrics
- Test: write_markdown_report creates file with expected header and table structure
- Test: write_markdown_report includes all metrics regardless of pass/fail
- Test: write_json_report creates valid JSON file
- Test: write_json_report has correct structure (overall_passed, cost, pld, dispatch keys)
- Test: report_validation returns (md_path, json_path) and creates both files
- Test: validate_against_reference with missing reference dir returns all-skipped ValidationResult

Use `mktempdir()` for file output tests. Capture console output using `IOBuffer()`.
  </action>
  <verify>
Run: `julia --project=test -e 'include("test/validation/test_reporters.jl")'`
All reporter tests pass. At least 30 @test assertions.
Verify Validation.jl module loads: `julia --project=test -e 'using OpenDESSEM; using OpenDESSEM.Analysis.Validation'`
  </verify>
  <done>
All three report formats generate correctly: console (sectioned summary), markdown (full comparison tables), JSON (structured mirror). validate_against_reference() orchestrates full validation pipeline. Validation module wired into Analysis and loadable from OpenDESSEM. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire Validation module into Analysis and update test runner</name>
  <files>
    src/analysis/Analysis.jl
    test/runtests.jl
  </files>
  <action>
**Update `src/analysis/Analysis.jl`:**

Add the Validation submodule to the Analysis module. Follow the existing pattern of how `SolutionExporter` is included:

1. Add `include("../validation/Validation.jl")` after the existing includes
2. Add `using .Validation` to re-export Validation's exports
3. Add all Validation exports to the Analysis export list

The Validation module imports `SolverResult` from `...Solvers` (three dots) and `ElectricitySystem` from `...OpenDESSEM` (three dots). Three dots are needed because Validation is nested inside Analysis, which is inside OpenDESSEM: `Validation -> Analysis -> OpenDESSEM -> Solvers`. There is NO `Core` submodule -- `ElectricitySystem` is exported directly from the top-level `OpenDESSEM` module. If three-dot import for `ElectricitySystem` has issues, use `Main.OpenDESSEM.ElectricitySystem` as fallback.

**Update `test/runtests.jl`:**

Add the validation test includes at the end, before the closing `end`:
```julia
# Validation tests
include("validation/test_validation_types.jl")
include("validation/test_reference_loader.jl")
include("validation/test_comparators.jl")
include("validation/test_reporters.jl")
```

Run the full test suite to verify no regressions.
  </action>
  <verify>
Run full test suite: `julia --project=test test/runtests.jl`
All 2075+ existing tests pass. All new validation tests pass. No regressions.
Verify module access: `julia --project=test -e 'using OpenDESSEM; using OpenDESSEM.Analysis; println(typeof(ValidationResult))'`
  </verify>
  <done>
Validation module fully wired into Analysis. All validation exports accessible via `using OpenDESSEM.Analysis`. Full test suite passes with no regressions. New validation tests integrated into runtests.jl.
  </done>
</task>

</tasks>

<verification>
1. `julia --project=test -e 'include("test/validation/test_reporters.jl")'` -- all pass
2. `julia --project=test test/runtests.jl` -- all tests pass including new validation tests
3. Console report has sectioned output (not verbose)
4. Markdown report has complete tables for ALL metrics
5. JSON report has structured output mirroring markdown
6. validate_against_reference() collects all failures (no early termination)
7. Module accessible: `using OpenDESSEM.Analysis.Validation`
</verification>

<success_criteria>
- Console report prints sectioned pass/fail summary per metric type
- Markdown report generates full comparison tables with expected vs actual for ALL metrics
- JSON report generates structured output mirroring markdown content
- validate_against_reference() orchestrates all comparisons without early termination
- report_validation() generates all three formats in a single call
- Validation module wired into Analysis and accessible from OpenDESSEM
- All 2075+ existing tests continue to pass
- 30+ new test assertions for reporters
</success_criteria>

<output>
After completion, create `.planning/phases/05-end-to-end-validation/05-03-SUMMARY.md`
</output>
